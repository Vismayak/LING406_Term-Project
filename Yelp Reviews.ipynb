{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit - Advanced Baseline System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to take the same baseline system developed previously and use it on Yelp Reviews to seperate the good reviews (or those that are 3.5 stars and above) and the bad reviews (less than 3.5 stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import the various python libraries that would be needed in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting lists from sentiment lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating negative lexicon\n",
    "with open(\"negative-words.txt\",encoding = \"ISO-8859-1\") as file:\n",
    "    content = file.readlines()\n",
    "content = [x.strip() for x in content] \n",
    "negative_words = [x for x in content if not x.startswith(\";\")]\n",
    "negative_words.remove('') # removing observed garbage value\n",
    "\n",
    "#creating positive lexicon\n",
    "with open(\"positive-words.txt\",encoding = \"ISO-8859-1\") as file:\n",
    "    content = file.readlines()\n",
    "content = [x.strip() for x in content] \n",
    "positive_words = [x for x in content if not x.startswith(\";\")]\n",
    "positive_words.remove('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a list of all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"yelp_reviews/all_reviews.txt\"\n",
    "\n",
    "with open(filename) as file:\n",
    "    text = file.read()\n",
    "\n",
    "reviews = text.split(\"]]]\")\n",
    "del reviews[10391] #removing a particular garbage value\n",
    "reviews[0] = '\\n'+reviews[0] # adding hashtag to keep consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate each review to return star rating and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_review(review):\n",
    "    stars =  review[5] \n",
    "    text = review[21:]\n",
    "    return int(stars),text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp_text=\"\"\n",
    "    text = text.lower()  #remove all upper cases\n",
    "    \n",
    "# the following feature has a very heavy weightage in accuracy\n",
    "#   helps remove punctuations\n",
    "    for i in text:\n",
    "        if i not in punctuation:\n",
    "             temp_text=temp_text +i\n",
    "    \n",
    "    text=temp_text\n",
    "    tokens = text.split()  #split list into elements \n",
    "    \n",
    "    #then remove all numerical text from the tokens\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # the next step is to remove all stop words from the text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text and updating the Vocabulary (The vocabulary needs to be constantly updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_vocab(tokens,vocab):    \n",
    "    vocab.update(tokens)\n",
    "#testing for this function\n",
    "# update_vocab(\"review_polarity/txt_sentoken/pos/cv199_9629.txt\")\n",
    "# print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting reviews into training and testing lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_review = reviews[:8500]\n",
    "testing_review = reviews[8500:]\n",
    "# print (len(training_review))\n",
    "# print (len(testing_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminating some of the minimum occurring words and saving the vocabulary as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_vocab(vocab):\n",
    "    minimum_occurance = 2\n",
    "    voc = [i for i,j in vocab.items() if j>= minimum_occurance]\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vocabulary using the previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A counter that would be used for vocabulary\n",
    "pos_vocab = Counter() # initialize the counter to be used throughout\n",
    "neg_vocab = Counter()\n",
    "\n",
    "for i in training_review:\n",
    "    stars,text = evaluate_review(i)\n",
    "    tokens = clean_text(text)\n",
    "    if stars > 3:\n",
    "        update_vocab(tokens,pos_vocab)\n",
    "    elif stars < 3:\n",
    "        update_vocab(tokens,neg_vocab)\n",
    "    elif stars == 3:\n",
    "        if ('3.5' in tokens) and ('stars' in tokens):\n",
    "            update_vocab(tokens,pos_vocab)\n",
    "        else:\n",
    "            update_vocab(tokens,neg_vocab)\n",
    "\n",
    "#Removing words that hardly occur\n",
    "pos_vocabulary= save_vocab(pos_vocab) #.most_common(5000))\n",
    "neg_vocabulary = save_vocab(neg_vocab) #.most_common(5000))\n",
    "#print(pos_vocabulary)\n",
    "\n",
    "\n",
    "# the following feature was remove because it greatly reduced accuracy\n",
    "#Removing words that are not in lexicon\n",
    "# pos_vocabulary = [i for i in pos_vocabulary if i in positive_words]\n",
    "# neg_vocabulary = [i for i in neg_vocabulary if i in negative_words]\n",
    "        \n",
    "# print(neg_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_review():\n",
    "    correct = 0\n",
    "    for i in testing_review:\n",
    "        stars,text = evaluate_review(i)\n",
    "        tokens = clean_text(i)\n",
    "        pos_decision =0\n",
    "        neg_decision = 0\n",
    "        for i in tokens:\n",
    "            if i in pos_vocabulary:\n",
    "                pos_decision += 1 #* pos_vocab[i]/pos_sum   # weights were taken off cos of reason stated above\n",
    "            if i in neg_vocabulary:\n",
    "                neg_decision +=1 #* neg_vocab[i]/neg_sum\n",
    "        if pos_decision >= neg_decision :\n",
    "            pos = 1\n",
    "        else:\n",
    "            pos = -1\n",
    "        if stars > 3:\n",
    "            if pos == 1:\n",
    "                correct += 1\n",
    "        elif stars < 3:\n",
    "            if pos == -1:\n",
    "                correct += 1\n",
    "        elif stars == 3:\n",
    "            if ('3.5' in tokens) and ('stars' in tokens):\n",
    "                if pos == 1:\n",
    "                    correct += 1\n",
    "            else:\n",
    "                if pos == -1:\n",
    "                    correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 76.25594923320995\n"
     ]
    }
   ],
   "source": [
    "number_of_corrects = test_review() #we are testing on the positive files\n",
    "accuracy =  number_of_corrects/len(testing_review) * 100\n",
    "\n",
    "print (\"Accuracy is \"+ str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
